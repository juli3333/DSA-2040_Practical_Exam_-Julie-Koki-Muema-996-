{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb89b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m## Prepare features\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Features only (exclude species column)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[iris\u001b[38;5;241m.\u001b[39mfeature_names]  \u001b[38;5;66;03m# normalized features from Task 1\u001b[39;00m\n\u001b[0;32m     11\u001b[0m y_true \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m'\u001b[39m]      \u001b[38;5;66;03m# actual class labels\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m## Apply K-Means clustering\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize K-Means with 3 clusters\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "## TASK 2: CLUSTERING\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "## Prepare features\n",
    "# Features only (exclude species column)\n",
    "X = df[iris.feature_names]  # normalized features from Task 1\n",
    "y_true = df['species']      # actual class labels\n",
    "\n",
    "\n",
    "## Apply K-Means clustering\n",
    "# Initialize K-Means with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# Fit the model to the features\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predict clusters\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# Show first 10 predicted clusters\n",
    "print(\"Predicted clusters:\", y_pred[:10])\n",
    "\n",
    "\n",
    "## Compare clusters with actual labels using ARI\n",
    "# Compute Adjusted Rand Index\n",
    "ari = adjusted_rand_score(y_true, y_pred)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef04779",
   "metadata": {},
   "source": [
    "- n_clusters=3 because the Iris dataset has 3 species.\n",
    "\n",
    "- fit() finds cluster centroids.\n",
    "\n",
    "- predict() assigns each sample to the nearest cluster.\n",
    "\n",
    "\n",
    "**ARI**\n",
    "- ARI evaluates how well the clustering matches the true species labels, adjusting for chance.\n",
    "\n",
    "- A higher ARI (close to 1) means clusters match classes very well.\n",
    "\n",
    "- ARI = 0 means random clustering; negative values indicate worse than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute inertia for k = 2 and 4\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature matrix (normalized features from preprocessing)\n",
    "X = df[iris.feature_names]\n",
    "\n",
    "# Try k=2, 3, 4\n",
    "k_values = [2, 3, 4]\n",
    "inertia_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Print inertia for each k\n",
    "for k, inertia in zip(k_values, inertia_values):\n",
    "    print(f\"k={k}, Inertia={inertia:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e940e",
   "metadata": {},
   "source": [
    "- kmeans.inertia_ measures the sum of squared distances from samples to their nearest cluster center.\n",
    "\n",
    "- Lower inertia = tighter clusters.\n",
    "\n",
    "- We check k=2, k=3, k=4 to see which k gives a good balance between cluster tightness and simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elbow Curve\n",
    "plt.plot(k_values, inertia_values, 'bo-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for determining optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623378e",
   "metadata": {},
   "source": [
    "- The elbow point is where adding more clusters stops significantly decreasing inertia.\n",
    "\n",
    "- This is usually considered the optimal number of clusters.\n",
    "\n",
    "- For Iris dataset, the elbow typically appears at k=3, which matches the 3 species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the clusters\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Use normalized features (from Task 1)\n",
    "X = df[iris.feature_names]\n",
    "\n",
    "# Fit K-Means with k=3 (optimal for Iris)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Scatter plot: Petal Length vs Petal Width\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(\n",
    "    X['petal length (cm)'], \n",
    "    X['petal width (cm)'], \n",
    "    c=y_pred, \n",
    "    cmap='viridis', \n",
    "    s=50\n",
    ")\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title('K-Means Clustering of Iris Dataset (k=3)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f95e3",
   "metadata": {},
   "source": [
    "The Iris dataset's K-Means clustering with k=3 clearly separates the clusters, especially when comparing the length and width of the petals.  The algorithm successfully captured the natural groupings in the data, as evidenced by the majority of points being appropriately sorted by species.  Because real-world data is rarely entirely separable, there are sometimes minor misclassifications when species overlap significantly, particularly between Iris versicolor and Iris virginica.  Strong congruence between projected clusters and actual species labels is confirmed by the Adjusted Rand Index (ARI), proving that K-Means is capable of identifying significant patterns on its own without supervision.  Similar clustering approaches are frequently employed in real-world applications, such as customer segmentation, where companies classify clients according to their purchase patterns in order to efficiently target marketing campaigns. Depending on how closely the synthetic distributions resemble actual clusters, different outcomes could have been obtained if synthetic data had been utilized in place of the original Iris dataset. Inadequately produced synthetic data may decrease cluster interpretability and increase misclassification. All things considered, this experiment shows how unsupervised learning may be used practically to find innate groupings in structured information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
